{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sofzcc/lab-lora-tuning-peft/blob/main/lab-lora-tuning-peft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Y40X0WCY_H"
      },
      "source": [
        "# Lab | Introduction to LoRA Tuning using PEFT from Hugging Face\n",
        "<!-- ### Fine-tune a Foundational Model effortlessly -->\n",
        "\n",
        "**Note:** This is more or less the same notebook you saw in the previous lesson, but that is ok. This is an LLM fine-tuning lab. In class we used a set of datasets and models, and in the labs you are required to change the LLMs models and the datasets including the pre-processing pipelines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCinY2l-upqy"
      },
      "source": [
        "# LoRA Tuning\n",
        "\n",
        "In this notebook you are being introduced to how to apply LoRA Tuning with the PEFT library to a pre-trained model.\n",
        "\n",
        "For a complete list of Models compatible with PEFT refer to their [documentation](https://huggingface.co/docs/peft/main/en/index#supported-methods).\n",
        "\n",
        "A short sample of models families available to be trained with PEFT are: Bloom, Llama, GPT-J, GPT-2, BERT... and more. Hugging Face is working hard to bring more Models to the Library.\n",
        "\n",
        "## Brief introduction to LoRA Tuning.\n",
        "LoRA is a re-parameterization technique. Its operation is simple, complex, and brilliant at the same time. It involves reducing the size of the matrices to be trained by dividing them in such a way that when multiplied, they yield the original matrix.\n",
        "\n",
        "The weights that are modified are those of the reduced matrices, not the original matrix. It's better visualized in an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp_7rR4Nx7My"
      },
      "source": [
        "![Matrices_LoRA.webp](data:image/webp;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oSLJkXty-t_"
      },
      "source": [
        "We have an original matrix of 50x50, which means we would have to modify about 2500 parameters. However, as we know, if we multiply two matrices of (2x50) and (50x2), we obtain a 50x50 matrix. Yet, these two matrices are formed by only 100 parameters each. In other words, for the reduced matrices, we need to modify a total of 200 parameters compared to the 2500 of the original matrix. This represents a 92% reduction, and the larger the original matrix, the greater the percentage of savings.\n",
        "\n",
        "In Language Models like GPT-3 or any of the current ones with LoRA, it's possible that we only need to train about 0.02% of the original parameters. This varies for each model. The best part is that the obtained result is very similar to that of full fine-tuning, in some cases, it can even be better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "620MdVMk7iUS"
      },
      "source": [
        "# Load the PEFT and Datasets Libraries.\n",
        "\n",
        "The PEFT library contains the Hugging Face implementation of differente fine-tuning techniques, like LoRA Tuning.\n",
        "\n",
        "Using the Datasets library we have acces to a huge amount of Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_UyyuMGnCPjA",
        "tags": [],
        "outputId": "796028e0-736e-46a6-b56b-c51a0301cee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipywidgets==7.7.5\n",
            "  Downloading ipywidgets-7.7.5-py2.py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.5) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.5) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.5) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.4 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.5) (3.6.6)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==7.7.5) (7.34.0)\n",
            "Collecting jupyterlab-widgets<3,>=1.0.0 (from ipywidgets==7.7.5)\n",
            "  Downloading jupyterlab_widgets-1.1.7-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.4/295.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.5) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.5) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.5) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets==7.7.5)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.5) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.5) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.5) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.5) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.5) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.5) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.5) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets==7.7.5) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (3.1.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets==7.7.5) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets==7.7.5) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets==7.7.5) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (4.2.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (24.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (4.19.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel>=4.5.1->ipywidgets==7.7.5) (1.16.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.18.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets==7.7.5) (1.2.1)\n",
            "Installing collected packages: jupyterlab-widgets, jedi, ipywidgets\n",
            "  Attempting uninstall: jupyterlab-widgets\n",
            "    Found existing installation: jupyterlab_widgets 3.0.11\n",
            "    Uninstalling jupyterlab_widgets-3.0.11:\n",
            "      Successfully uninstalled jupyterlab_widgets-3.0.11\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed ipywidgets-7.7.5 jedi-0.19.1 jupyterlab-widgets-1.1.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ipywidgets"
                ]
              },
              "id": "1158607d2f8749afb9c8184e39d710b7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -q peft==0.8.2\n",
        "!pip install -q datasets==2.16.1\n",
        "!pip install ipywidgets==7.7.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOnJlBY-81Wl"
      },
      "source": [
        "From the transformers library we import the necesary classes to import the model and the tokenizer.\n",
        "\n",
        "Then we can load the Tokenizer and the model.\n",
        "\n",
        "Bloom is one of the smallest and smarter model available to be trained with PEFT Library using Prompt Tuning. You can use either of the models in the Bloom Family, I encorage you to use at least two of them and see the differences.\n",
        "\n",
        "I'm using the smallest one just to spend less time trainig, and avoid memory problems in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vziwd2UuCYGl",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name =  \"bigscience/bloomz-560m\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "foundation_model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtc1gbK39Hp7"
      },
      "source": [
        "## Inference with the pre-trained model.\n",
        "I'm going to do a test with the pre-trained model without fine-tuning, to see if something changes after the fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jak6FzpvFTHk",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#this function returns the outputs from the model received, and inputs.\n",
        "def get_outputs(model, inputs, max_new_tokens=100): #play with this function inputs and see if you get something interesting\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        repetition_penalty=1.5, #Avoid repetition.\n",
        "        early_stopping=True, #The model can stop before reach the max_length\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this function returns the outputs from the model received, and inputs.\n",
        "def get_outputs2(model, inputs, max_new_tokens=100, top_k=50): # top-K sampling to restrict the model to consider only the top K most likely next tokens at each step\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        repetition_penalty=1.5, #Avoid repetition.\n",
        "        early_stopping=True, #The model can stop before reach the max_length\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        top_k = top_k\n",
        "    )\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "K_cRaNtNDBns"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XP1QfWP5IFxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkFqjS459jAa"
      },
      "source": [
        "The dataset used for the fine-tuning contains prompts to be used with Large Language Models.\n",
        "\n",
        "I'm going to request the pre-trained model that acts like a motivational coach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BAYg7czFYeK",
        "outputId": "9d852453-6c9f-406f-9c88-962f3cb93957",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I want you to act as a professional and experienced screenwriter.']\n"
          ]
        }
      ],
      "source": [
        "#Inference original model\n",
        "input_sentences = tokenizer(\"I want you to act as a professional and experienced screenwriter.\", return_tensors=\"pt\")\n",
        "foundational_outputs_sentence = get_outputs(foundation_model, input_sentences, max_new_tokens=400)\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Inference test model\n",
        "input_sentences = tokenizer(\"I want you to act as a professional and experienced screenwriter.\", return_tensors=\"pt\")\n",
        "foundational_outputs_sentence = get_outputs2(foundation_model, input_sentences, max_new_tokens=100)\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "RhNiw9qBDp1r",
        "outputId": "2a33d3be-80ce-4927-9d21-4fa448a5b22a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I want you to act as a professional and experienced screenwriter.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQUGY47p9ysI"
      },
      "source": [
        "Not sure if the answer is correct or not, but for sure is not a prompt. We need to train our model if we want that acts like a prompt engineer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL5L_DcR9ggA"
      },
      "source": [
        "# Preparing the Dataset.\n",
        "The Dataset used is:\n",
        "\n",
        "https://huggingface.co/datasets/fka/awesome-chatgpt-prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "DyIMQ7IHFbIx",
        "outputId": "ee8350df-4a7a-44de-859f-930128d89e4d",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = \"fka/awesome-chatgpt-prompts\"\n",
        "\n",
        "#Create the Dataset to create prompts.\n",
        "data = load_dataset(dataset)\n",
        "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "train_sample = data[\"train\"].select(range(50))\n",
        "\n",
        "train_sample = train_sample.remove_columns('act')\n",
        "\n",
        "display(train_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmlZY3fk_9fm",
        "outputId": "6729395d-382c-4614-b8fb-7c15ea86d39d",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': ['I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd'], 'input_ids': [[44, 4026, 1152, 427, 1769, 661, 267, 104105, 28434, 17, 473, 2152, 4105, 49123, 530, 1152, 2152, 57502, 1002, 3595, 368, 28434, 3403, 6460, 17, 473, 4026, 1152, 427, 3804, 57502, 1002, 368, 28434, 10014, 14652, 2592, 19826, 4400, 10973, 15, 530, 16915, 4384, 17, 727, 1130, 11602, 184637, 17, 727, 1130, 4105, 49123, 35262, 473, 32247, 1152, 427, 727, 1427, 17, 3262, 707, 3423, 427, 13485, 1152, 7747, 361, 170205, 15, 707, 2152, 727, 1427, 1331, 55385, 5484, 14652, 6291, 999, 117805, 731, 29726, 1119, 96, 17, 2670, 3968, 9361, 632, 269, 42512]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "print(train_sample[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVPAJsrUAHiJ"
      },
      "source": [
        "# Fine-Tuning.\n",
        "First is necesary create a LoRA config.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwAK6kxCDCfM"
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "uCalslQFGL7K"
      },
      "outputs": [],
      "source": [
        "# TARGET_MODULES\n",
        "# https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220\n",
        "\n",
        "import peft\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "lora_config = LoraConfig(#play with these config inputs\n",
        "    r=4, #As bigger the R bigger the parameters to train.\n",
        "    lora_alpha=1, # a scaling factor that adjusts the magnitude of the weight matrix. Usually set to 1\n",
        "    target_modules=[\"query_key_value\"], #You can obtain a list of target modules in the URL above.\n",
        "    lora_dropout=0.05, #Helps to avoid Overfitting.\n",
        "    bias=\"lora_only\", # this specifies if the bias parameter should be trained.\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUddynl0B1Ck"
      },
      "source": [
        "The most important parameter is **r**, it defines how many parameters will be trained. As bigger the valuer more parameters are trained, but it means that the model will be able to learn more complicated relations between input and output.\n",
        "\n",
        "Yo can find a list of the **target_modules** available on the [Hugging Face Documentation]( https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220)\n",
        "\n",
        "**lora_dropout** is like the commom dropout is used to avoid overfitting.\n",
        "\n",
        "**bias** I was hesitating if use *none* or *lora_only*. For text classification the most common value is none, and for chat or question answering, *all* or *lora_only*.\n",
        "\n",
        "**task_type**. Indicates the task the model is beign trained for. In this case, text generation.\n",
        "\n",
        "### Create the PEFT model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BG1zBmhsGQ-h",
        "outputId": "4d73c019-054d-4a5e-d4b4-7ae902f637e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 393,216 || all params: 559,607,808 || trainable%: 0.07026635339584111\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "peft_model = get_peft_model(foundation_model, lora_config)\n",
        "print(peft_model.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import peft\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Define ranges for parameters\n",
        "r_values = [1, 3, 7]\n",
        "lora_dropout_values = [0.1, 0.3, 0.5]\n",
        "\n",
        "for r in r_values:\n",
        "    for lora_dropout in lora_dropout_values:\n",
        "        lora_config_2 = LoraConfig(\n",
        "            r=r,\n",
        "            lora_alpha=1,\n",
        "            target_modules=[\"query_key_value\"],\n",
        "            lora_dropout=lora_dropout,\n",
        "            bias=\"lora_only\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "\n",
        "        peft_model2 = get_peft_model(foundation_model,lora_config_2)\n",
        "        print(f\"Parameters r = {r} and lora_dropout = {lora_dropout} \\n\", peft_model2.print_trainable_parameters())\n"
      ],
      "metadata": {
        "id": "7GD5rLAJJ4tS",
        "outputId": "9cca603e-a98e-4b33-923a-e1411797dceb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 98,304 || all params: 559,312,896 || trainable%: 0.01757585078102687\n",
            "Parameters r = 1 and lora_dropout = 0.1 \n",
            " None\n",
            "trainable params: 98,304 || all params: 559,312,896 || trainable%: 0.01757585078102687\n",
            "Parameters r = 1 and lora_dropout = 0.3 \n",
            " None\n",
            "trainable params: 98,304 || all params: 559,312,896 || trainable%: 0.01757585078102687\n",
            "Parameters r = 1 and lora_dropout = 0.5 \n",
            " None\n",
            "trainable params: 294,912 || all params: 559,509,504 || trainable%: 0.05270902422418905\n",
            "Parameters r = 3 and lora_dropout = 0.1 \n",
            " None\n",
            "trainable params: 294,912 || all params: 559,509,504 || trainable%: 0.05270902422418905\n",
            "Parameters r = 3 and lora_dropout = 0.3 \n",
            " None\n",
            "trainable params: 294,912 || all params: 559,509,504 || trainable%: 0.05270902422418905\n",
            "Parameters r = 3 and lora_dropout = 0.5 \n",
            " None\n",
            "trainable params: 688,128 || all params: 559,902,720 || trainable%: 0.12290134972017996\n",
            "Parameters r = 7 and lora_dropout = 0.1 \n",
            " None\n",
            "trainable params: 688,128 || all params: 559,902,720 || trainable%: 0.12290134972017996\n",
            "Parameters r = 7 and lora_dropout = 0.3 \n",
            " None\n",
            "trainable params: 688,128 || all params: 559,902,720 || trainable%: 0.12290134972017996\n",
            "Parameters r = 7 and lora_dropout = 0.5 \n",
            " None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**: It's interesting to see that a higher r increases the percentage of trainable parameters, while lora_dropout doesn't influence it."
      ],
      "metadata": {
        "id": "XhTnlqUbKpaj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2onXUsaw-Ga0"
      },
      "source": [
        "The number of trainable parameters is really small compared with the total number of parameters in the pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "HArPQ_lvGUkY"
      },
      "outputs": [],
      "source": [
        "#Create a directory to contain the Model\n",
        "import os\n",
        "working_dir = './'\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWalmqWm4STo"
      },
      "source": [
        "In the TrainingArgs we inform the number of epochs we want to train, the output directory and the learning_rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ND0aJ-t6ARqD"
      },
      "outputs": [],
      "source": [
        "#Creating the TrainingArgs\n",
        "import transformers\n",
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_directory,\n",
        "    auto_find_batch_size=True, # Find a correct bvatch size that fits the size of Data.\n",
        "    learning_rate= 3e-2, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=2,\n",
        "    use_cpu=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgxsV-iy_J_o"
      },
      "source": [
        "Now we can train the model.\n",
        "To train the model we need:\n",
        "\n",
        "\n",
        "*   The PEFT Model.\n",
        "*   The training_args\n",
        "* The Dataset\n",
        "* The result of DataCollator, the Dataset ready to be procesed in blocks.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "z5NYHqBnGZyF",
        "outputId": "9cfceb2d-795c-4ecc-9036-ce5e68bbddce"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14/14 14:15, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=14, training_loss=2.7346507481166293, metrics={'train_runtime': 925.8693, 'train_samples_per_second': 0.108, 'train_steps_per_second': 0.015, 'total_flos': 20092642099200.0, 'train_loss': 2.7346507481166293, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "#This cell may take up to 15 minutes to execute.\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_sample,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "kEKiFdpDGgOx",
        "outputId": "a4742fcf-6054-4f03-c675-cbfeb9ea17f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Save the model.\n",
        "peft_model_path = os.path.join(output_directory, f\"lora_model\")\n",
        "\n",
        "trainer.model.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "_TAjrSWSe14q"
      },
      "outputs": [],
      "source": [
        "#Load the Model.\n",
        "loaded_model = PeftModel.from_pretrained(foundation_model,\n",
        "                                        peft_model_path,\n",
        "                                        is_trainable=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK--YFPR6OxH"
      },
      "source": [
        "## Inference the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_27uvJudf03",
        "outputId": "4d6dae4e-c937-48fd-ea10-7ecb606b0201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I want you to act as an experienced screenwriter. I will provide your knowledge of different types and styles in the field, including animation sequences for films such as: motion picture short stories; feature-length documentary series featuring actors or actresses with roles similar enough that they can be easily adapted into other genres (']\n"
          ]
        }
      ],
      "source": [
        "input_sentences = tokenizer(\"I want you to act as an experienced screenwriter\", return_tensors=\"pt\")\n",
        "foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCV9JOBG6Ug8"
      },
      "source": [
        "The result is amazing. Let's compare the answer of the pre-trained Model withe the one fine-tuned by us using LoRA:\n",
        "* **Pretrained Model:** *I want you to act as a motivational coach.*  Don't be afraid of being challenged.\n",
        "* **Fine-Tuned Model:** I want you to act as a motivational coach.  I will provide some information about someone\\'s motivation and goals, but it should be your job  in order my first request – \"I need someone who can help me find the best way for myself stay motivated when competing against others.\" My suggestion is “I have\n",
        "\n",
        "As you can see the result is really similar to the samples containmed in the Datased used to fine-tune the Model. And we only trained the Model for 10 epochs and with a really small number of rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr4Sm32y89Ji"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "- Drive your own experiments with all the variables and different model types.\n",
        "    - Please with the **lora_config** values, maybe you can achieve a better result in less epochs, saving time and money for your company. :-)\n",
        "- Write a one page report\n",
        "    - What did you learn?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "96kLN29AChdw",
        "outputId": "ee8dcda5-77fc-4412-ac22-96e10fe60f58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14/14 13:38, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I want you to act as an experienced screenwriter. I will write a script for your character, and then create the storyline based on it using different elements of dialogue between characters in order that they connect with each other better than others.  My first request is \"I need help writing my own novel about']\n"
          ]
        }
      ],
      "source": [
        "# TARGET_MODULES\n",
        "# https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220\n",
        "\n",
        "import peft\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=7, #Increased r to 7\n",
        "    lora_alpha=1,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"lora_only\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "peft_model2 = get_peft_model(foundation_model,lora_config_2)\n",
        "\n",
        "import os\n",
        "working_dir = './'\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs_2\")\n",
        "\n",
        "\n",
        "import transformers\n",
        "from transformers import TrainingArguments, Trainer\n",
        "training_args2 = TrainingArguments(\n",
        "    output_dir=output_directory,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate= 3e-2,\n",
        "    num_train_epochs=2,\n",
        "    use_cpu=True\n",
        ")\n",
        "\n",
        "\n",
        "trainer2 = Trainer(\n",
        "    model=peft_model2,\n",
        "    args=training_args2,\n",
        "    train_dataset=train_sample,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "trainer2.train()\n",
        "\n",
        "#Save the model.\n",
        "peft_model_path2 = os.path.join(output_directory, f\"lora_model2\")\n",
        "\n",
        "trainer2.model.save_pretrained(peft_model_path2)\n",
        "\n",
        "#Load the Model.\n",
        "loaded_model2 = PeftModel.from_pretrained(foundation_model,\n",
        "                                        peft_model_path2,\n",
        "                                        is_trainable=False)\n",
        "\n",
        "input_sentences2 = tokenizer(\"I want you to act as an experienced screenwriter\", return_tensors=\"pt\")\n",
        "foundational_outputs_sentence2 = get_outputs(loaded_model2, input_sentences2, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence2, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output 1 (r=4)**\n",
        "\n",
        "\"['I want you to act as an experienced screenwriter. I will provide your knowledge of different types and styles in the field, including animation sequences for films such as: motion picture short stories; feature-length documentary series featuring actors or actresses with roles similar enough that they can be easily adapted into other genres (']\n",
        "\"\n",
        " Emphasizes providing knowledge and examples related to different types and styles in screenwriting, including specific genres and film techniques.\n",
        "\n",
        "**Output 2 (r=7)**\n",
        "\n",
        "\"['I want you to act as an experienced screenwriter. I will write a script for your character, and then create the storyline based on it using different elements of dialogue between characters in order that they connect with each other better than others.  My first request is \"I need help writing my own novel about']\"\n",
        "\n",
        "Focuses on the practical aspects of screenwriting, such as scriptwriting for characters and developing storylines through character interactions and dialogue.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Adjusting r and observing its impact on generated outputs provides insights into optimizing model performance for diverse applications.\n",
        "The difference in r values between the two outputs showcases how parameter tuning in LoraConfig influences the generated outputs and can be adjusted to align with specific requirements in screenwriting or other creative tasks."
      ],
      "metadata": {
        "id": "1XtBi5BLWjHf"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}